{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run config file\n",
    "import os\n",
    "os.system('python Config.py')\n",
    "os.system('python GlobalsVars.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "import GlobalsVars as v\n",
    "import glob\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start gs_1 generation\n"
     ]
    }
   ],
   "source": [
    "print (\"Start gs_1 generation\")\n",
    "# folder gs_1\n",
    "# Feature agglomeration with median window 2s\n",
    "path = 'gs_1/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "if not os.path.exists(path + '/arousal/'):\n",
    "    os.makedirs(path + '/arousal/')\n",
    "    os.makedirs(path + '/valence/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglo_fn(X):\n",
    "    from sklearn.cluster import FeatureAgglomeration\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    if X.shape != (7501,6):\n",
    "        X = np.transpose(X)\n",
    "    \n",
    "    agglo=FeatureAgglomeration(n_clusters=1).fit_transform(X)\n",
    "    return agglo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median filter\n",
    "def mfilt(x,size = 2):\n",
    "    \n",
    "    from scipy.signal import medfilt\n",
    "    winmed = medfilt(x,25*size-1)\n",
    "    \n",
    "    return winmed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rating individual files of train and dev on arousal and valence\n",
    "def listFiles():\n",
    "    files = []\n",
    "    #We get a tab countaining each file\n",
    "    for i, s in enumerate(v.agsi):\n",
    "        files.append([sorted(filter( lambda f: not f.startswith('.'), os.listdir(s+\".\"))),s])\n",
    "    return files;\n",
    "#End listFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write generated gold standard gs_1\n",
    "\n",
    "basedir = path \n",
    "header = \"\"\"@relation GOLDSTANDARD\n",
    "\n",
    "@attribute Instance_name string\n",
    "@attribute frameTime numeric\n",
    "@attribute GoldStandard numeric\n",
    "\n",
    "\n",
    "@data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "files = listFiles()\n",
    "\n",
    "#X = np.zeros(6,7501)\n",
    "namespace = globals()\n",
    "for i in range(len(v.eName)/2):\n",
    "    for f in files[i][0]:\n",
    "        #print(files[i][1]+f)\n",
    "        #print((files[i][1]+f).replace(\"arousal\",\"valence\"))\n",
    "        df_arr = pd.read_csv(files[i][1]+f,delimiter=';')\n",
    "        df_val = pd.read_csv((files[i][1]+f).replace(\"arousal\",\"valence\"),delimiter=';')\n",
    "        fn = os.path.splitext(f)[0]\n",
    "#         namespace['X_arr_%s' %fn] = df_arr.iloc[:,1:7]\n",
    "#         namespace['X_val_%s'%fn] = df_val.iloc[:,1:7]\n",
    "        \n",
    "        X_arr = np.array((mfilt(df_arr.iloc[:,1]),mfilt(df_arr.iloc[:,2]),\\\n",
    "                                     mfilt(df_arr.iloc[:,3]),mfilt(df_arr.iloc[:,4]),\\\n",
    "                                     mfilt(df_arr.iloc[:,5]),mfilt(df_arr.iloc[:,6])))\n",
    "        X_val = np.array((mfilt(df_val.iloc[:,1]),mfilt(df_val.iloc[:,2]),\\\n",
    "                                     mfilt(df_val.iloc[:,3]),mfilt(df_val.iloc[:,4]),\\\n",
    "                                     mfilt(df_val.iloc[:,5]),mfilt(df_val.iloc[:,6])))\n",
    "        \n",
    "#         [pca_aro, spca_aro] = spca_fn(X_arr)\n",
    "#         [pca_val, spca_val] = spca_fn(X_val)\n",
    "        aggo_aro = agglo_fn(X_arr)\n",
    "        aggo_val = agglo_fn(X_val)\n",
    "    \n",
    "        arff_arr=open(basedir+\"arousal/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_arr.write(header)\n",
    "        for j in range(len(aggo_aro)):\n",
    "            arff_arr.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_arr[\"time\"].iloc[j])+\",\"+str(aggo_aro[j,0]))\n",
    "        arff_arr.close()\n",
    "        \n",
    "        arff_val=open(basedir+\"valence/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_val.write(header)\n",
    "        for j in range(len(aggo_val)):\n",
    "            arff_val.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_val[\"time\"].iloc[j])+\",\"+str(aggo_val[j,0]))\n",
    "        arff_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start gs_2 generation\n"
     ]
    }
   ],
   "source": [
    "print (\"Start gs_2 generation\")\n",
    "# folder gs_2\n",
    "# Feature agglomeration with flat window 6s\n",
    "path = 'gs_2/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "if not os.path.exists(path + '/arousal/'):\n",
    "    os.makedirs(path + '/arousal/')\n",
    "    os.makedirs(path + '/valence/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat and hanning window\n",
    "def smooth(x,window_len=25*6,window='flat'):\n",
    "\n",
    "    import numpy\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError, \"smooth only accepts 1 dimension arrays.\"\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError, \"Input vector needs to be bigger than window size.\"\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "\n",
    "\n",
    "    s=numpy.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=numpy.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('numpy.'+window+'(window_len)')\n",
    "        \n",
    "    y=numpy.convolve(w/w.sum(),x,mode='same')    \n",
    "    #y=numpy.convolve(w/w.sum(),s,mode='same')\n",
    "    return y#[(window_len/2-1):-(window_len/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write generated gold standard gs_2\n",
    "\n",
    "basedir = path \n",
    "header = \"\"\"@relation GOLDSTANDARD\n",
    "\n",
    "@attribute Instance_name string\n",
    "@attribute frameTime numeric\n",
    "@attribute GoldStandard numeric\n",
    "\n",
    "\n",
    "@data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "files = listFiles()\n",
    "\n",
    "#X = np.zeros(6,7501)\n",
    "namespace = globals()\n",
    "for i in range(len(v.eName)/2):\n",
    "    for f in files[i][0]:\n",
    "        #print(files[i][1]+f)\n",
    "        #print((files[i][1]+f).replace(\"arousal\",\"valence\"))\n",
    "        df_arr = pd.read_csv(files[i][1]+f,delimiter=';')\n",
    "        df_val = pd.read_csv((files[i][1]+f).replace(\"arousal\",\"valence\"),delimiter=';')\n",
    "        fn = os.path.splitext(f)[0]\n",
    "        \n",
    "        X_arr = np.array((smooth(df_arr.iloc[:,1]),smooth(df_arr.iloc[:,2]),\\\n",
    "                                     smooth(df_arr.iloc[:,3]),smooth(df_arr.iloc[:,4]),\\\n",
    "                                     smooth(df_arr.iloc[:,5]),smooth(df_arr.iloc[:,6])))\n",
    "        X_val = np.array((smooth(df_val.iloc[:,1]),smooth(df_val.iloc[:,2]),\\\n",
    "                                     smooth(df_val.iloc[:,3]),smooth(df_val.iloc[:,4]),\\\n",
    "                                     smooth(df_val.iloc[:,5]),smooth(df_val.iloc[:,6]))) \n",
    "        \n",
    "#         [pca_aro, spca_aro] = spca_fn(X_arr)\n",
    "#         [pca_val, spca_val] = spca_fn(X_val)\n",
    "        \n",
    "        \n",
    "        aggo_aro = agglo_fn(X_arr)\n",
    "        aggo_val = agglo_fn(X_val)\n",
    "    \n",
    "        arff_arr=open(basedir+\"arousal/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_arr.write(header)\n",
    "        for j in range(len(aggo_aro)):\n",
    "            arff_arr.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_arr[\"time\"].iloc[j])+\",\"+str(aggo_aro[j,0]))\n",
    "        arff_arr.close()\n",
    "        \n",
    "        arff_val=open(basedir+\"valence/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_val.write(header)\n",
    "        for j in range(len(aggo_val)):\n",
    "            arff_val.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_val[\"time\"].iloc[j])+\",\"+str(aggo_val[j,0]))\n",
    "        arff_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start gs_3 generation\n"
     ]
    }
   ],
   "source": [
    "print (\"Start gs_3 generation\")\n",
    "# folder gs_3\n",
    "# Sparse PCA with median window 2s\n",
    "path = 'gs_3/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "if not os.path.exists(path + '/arousal/'):\n",
    "    os.makedirs(path + '/arousal/')\n",
    "    os.makedirs(path + '/valence/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def mag(X_r):\n",
    "    from scipy.stats import zscore\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "    max_abs_scaler = MaxAbsScaler()\n",
    "    z_X_r = zscore(X_r)\n",
    "    z_X_r = max_abs_scaler.fit_transform(z_X_r)\n",
    "    return z_X_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse PCA\n",
    "def spca_fn(X):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.decomposition import SparsePCA\n",
    "    if X.shape != (7501,6):\n",
    "        X = np.transpose(X)\n",
    "    \n",
    "    pca = PCA(n_components=1)\n",
    "    X_r = pca.fit(X).transform(X)\n",
    "    spca = SparsePCA(n_components=1)\n",
    "    X_r2 = spca.fit(X).transform(X)\n",
    "    \n",
    "    return X_r,X_r2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write generated gold standard gs_3\n",
    "\n",
    "basedir= path\n",
    "header=\"\"\"@relation GOLDSTANDARD\n",
    "\n",
    "@attribute Instance_name string\n",
    "@attribute frameTime numeric\n",
    "@attribute GoldStandard numeric\n",
    "\n",
    "\n",
    "@data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "files = listFiles()\n",
    "\n",
    "#X = np.zeros(6,7501)\n",
    "namespace = globals()\n",
    "for i in range(len(v.eName)/2):\n",
    "    for f in files[i][0]:\n",
    "        #print(files[i][1]+f)\n",
    "        #print((files[i][1]+f).replace(\"arousal\",\"valence\"))\n",
    "        df_arr = pd.read_csv(files[i][1]+f,delimiter=';')\n",
    "        df_val = pd.read_csv((files[i][1]+f).replace(\"arousal\",\"valence\"),delimiter=';')\n",
    "        fn = os.path.splitext(f)[0]\n",
    "#         namespace['X_arr_%s' %fn] = df_arr.iloc[:,1:7]\n",
    "#         namespace['X_val_%s'%fn] = df_val.iloc[:,1:7]\n",
    "        \n",
    "        X_arr = np.array((mfilt(df_arr.iloc[:,1]),mfilt(df_arr.iloc[:,2]),\\\n",
    "                                     mfilt(df_arr.iloc[:,3]),mfilt(df_arr.iloc[:,4]),\\\n",
    "                                     mfilt(df_arr.iloc[:,5]),mfilt(df_arr.iloc[:,6])))\n",
    "        X_val = np.array((mfilt(df_val.iloc[:,1]),mfilt(df_val.iloc[:,2]),\\\n",
    "                                     mfilt(df_val.iloc[:,3]),mfilt(df_val.iloc[:,4]),\\\n",
    "                                     mfilt(df_val.iloc[:,5]),mfilt(df_val.iloc[:,6])))  \n",
    "        \n",
    "        [pca_aro, spca_aro] = spca_fn(X_arr)\n",
    "        [pca_val, spca_val] = spca_fn(X_val)\n",
    "        pca_aro = mag(pca_aro)\n",
    "        spca_aro = mag(spca_aro)\n",
    "        pca_val = mag(pca_val)\n",
    "        spca_val = mag(spca_val)\n",
    "    \n",
    "        arff_arr=open(basedir+\"arousal/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_arr.write(header)\n",
    "        for j in range(len(spca_aro)):\n",
    "            arff_arr.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_arr[\"time\"].iloc[j])+\",\"+str(spca_aro[j,0]))\n",
    "        arff_arr.close()\n",
    "        \n",
    "        arff_val=open(basedir+\"valence/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_val.write(header)\n",
    "        for j in range(len(spca_val)):\n",
    "            arff_val.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_val[\"time\"].iloc[j])+\",\"+str(spca_val[j,0]))\n",
    "        arff_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start gs_4 generation\n"
     ]
    }
   ],
   "source": [
    "print (\"Start gs_4 generation\")\n",
    "# folder gs_4\n",
    "# Agglormeration with median window 6s\n",
    "path = 'gs_4/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "if not os.path.exists(path + '/arousal/'):\n",
    "    os.makedirs(path + '/arousal/')\n",
    "    os.makedirs(path + '/valence/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median filter\n",
    "def mfilt(x,size = 6):\n",
    "    \n",
    "    from scipy.signal import medfilt\n",
    "    winmed = medfilt(x,25*size-1)\n",
    "    \n",
    "    return winmed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write generated gold standard gs_4\n",
    "\n",
    "basedir = path \n",
    "header = \"\"\"@relation GOLDSTANDARD\n",
    "\n",
    "@attribute Instance_name string\n",
    "@attribute frameTime numeric\n",
    "@attribute GoldStandard numeric\n",
    "\n",
    "\n",
    "@data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "files = listFiles()\n",
    "\n",
    "#X = np.zeros(6,7501)\n",
    "namespace = globals()\n",
    "for i in range(len(v.eName)/2):\n",
    "    for f in files[i][0]:\n",
    "        #print(files[i][1]+f)\n",
    "        #print((files[i][1]+f).replace(\"arousal\",\"valence\"))\n",
    "        df_arr = pd.read_csv(files[i][1]+f,delimiter=';')\n",
    "        df_val = pd.read_csv((files[i][1]+f).replace(\"arousal\",\"valence\"),delimiter=';')\n",
    "        fn = os.path.splitext(f)[0]\n",
    "#         namespace['X_arr_%s' %fn] = df_arr.iloc[:,1:7]\n",
    "#         namespace['X_val_%s'%fn] = df_val.iloc[:,1:7]\n",
    "        \n",
    "        X_arr = np.array((mfilt(df_arr.iloc[:,1]),mfilt(df_arr.iloc[:,2]),\\\n",
    "                                     mfilt(df_arr.iloc[:,3]),mfilt(df_arr.iloc[:,4]),\\\n",
    "                                     mfilt(df_arr.iloc[:,5]),mfilt(df_arr.iloc[:,6])))\n",
    "        X_val = np.array((mfilt(df_val.iloc[:,1]),mfilt(df_val.iloc[:,2]),\\\n",
    "                                     mfilt(df_val.iloc[:,3]),mfilt(df_val.iloc[:,4]),\\\n",
    "                                     mfilt(df_val.iloc[:,5]),mfilt(df_val.iloc[:,6])))\n",
    "        \n",
    "#         [pca_aro, spca_aro] = spca_fn(X_arr)\n",
    "#         [pca_val, spca_val] = spca_fn(X_val)\n",
    "        aggo_aro = agglo_fn(X_arr)\n",
    "        aggo_val = agglo_fn(X_val)\n",
    "    \n",
    "        arff_arr=open(basedir+\"arousal/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_arr.write(header)\n",
    "        for j in range(len(aggo_aro)):\n",
    "            arff_arr.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_arr[\"time\"].iloc[j])+\",\"+str(aggo_aro[j,0]))\n",
    "        arff_arr.close()\n",
    "        \n",
    "        arff_val=open(basedir+\"valence/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_val.write(header)\n",
    "        for j in range(len(aggo_val)):\n",
    "            arff_val.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_val[\"time\"].iloc[j])+\",\"+str(aggo_val[j,0]))\n",
    "        arff_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start gs_5 generation\n"
     ]
    }
   ],
   "source": [
    "print (\"Start gs_5 generation\")\n",
    "# folder gs_5\n",
    "# with distribution regularization\n",
    "path = 'gs_5/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "if not os.path.exists(path + '/arousal/'):\n",
    "    os.makedirs(path + '/arousal/')\n",
    "    os.makedirs(path + '/valence/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training distribution\n",
    "arousal_gd_train_path = 'pre_train/arousal/train*.arff'\n",
    "arousal_gd_dev_path = 'pre_train/arousal/dev*.arff'\n",
    "valence_gd_train_path = 'pre_train/valence/train*.arff'\n",
    "valence_gd_dev_path = 'pre_train/valence/dev*.arff'\n",
    "\n",
    "agdtrfname = sorted(glob.glob(arousal_gd_train_path))\n",
    "arousal_gd_train = []\n",
    "for f in agdtrfname:\n",
    "    temp = pd.read_csv(f, sep=';')\n",
    "    temp = temp['@relation GOLDSTANDARD_AROUSAL'].apply(lambda x: x.split(',')[-1])\n",
    "    temp = temp[4:]\n",
    "    temp = pd.to_numeric(temp)\n",
    "    #print(f)\n",
    "    arousal_gd_train.extend(temp)\n",
    "    \n",
    "vgdtrfname = sorted(glob.glob(valence_gd_train_path))\n",
    "valence_gd_train = []\n",
    "for fv in vgdtrfname:\n",
    "    tempv = pd.read_csv(fv, sep=';')\n",
    "    tempv = tempv['@relation GOLDSTANDARD_VALENCE'].apply(lambda x: x.split(',')[-1])\n",
    "    tempv = tempv[4:]\n",
    "    tempv = pd.to_numeric(tempv)\n",
    "    #print(fv)\n",
    "    valence_gd_train.extend(tempv) \n",
    "\n",
    "agdevfname = sorted(glob.glob(arousal_gd_dev_path))\n",
    "arousal_gd_dev = []\n",
    "for fd in agdevfname:\n",
    "    temp = pd.read_csv(fd, sep=';')\n",
    "    temp = temp['@relation GOLDSTANDARD_AROUSAL'].apply(lambda x: x.split(',')[-1])\n",
    "    temp = temp[4:]\n",
    "    temp = pd.to_numeric(temp)\n",
    "    #print(fd)\n",
    "    arousal_gd_dev.extend(temp)\n",
    "    \n",
    "vgdevfname = sorted(glob.glob(valence_gd_dev_path))\n",
    "valence_gd_dev = []\n",
    "for fdv in vgdevfname:\n",
    "    tempv = pd.read_csv(fdv, sep=';')\n",
    "    tempv = tempv['@relation GOLDSTANDARD_VALENCE'].apply(lambda x: x.split(',')[-1])\n",
    "    tempv = tempv[4:]\n",
    "    tempv = pd.to_numeric(tempv)\n",
    "    #print(fdv)\n",
    "    valence_gd_dev.extend(tempv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "val_gd_data = valence_gd_train\n",
    "aro_gd_data = arousal_gd_train\n",
    "\n",
    "del valence_gd_train,arousal_gd_train\n",
    "\n",
    "X_train = np.array([val_gd_data,aro_gd_data])\n",
    "X_train = np.transpose(X_train)\n",
    "  \n",
    "X_test = np.array([valence_gd_dev,arousal_gd_dev])\n",
    "X_test = np.transpose(X_test)\n",
    "\n",
    "# Generate some abnormal novel observations\n",
    "# ran = np.random.uniform(low=-1, high=1, size=(1000, 1))\n",
    "# aro_out = np.setdiff1d(ran, aro_gd_data)\n",
    "# aro_out = np.setdiff1d(aro_out, aro_gd_dev_data)\n",
    "# val_out = np.setdiff1d(ran, val_gd_data)\n",
    "# val_out = np.setdiff1d(val_out, val_gd_dev_data)\n",
    "# aro_outliers = np.random.choice(aro_out, 50, replace=False)\n",
    "# val_outliers = np.random.choice(val_out, 50, replace=False)\n",
    "\n",
    "# X_outliers = np.array([val_outliers,aro_outliers])\n",
    "# X_outliers = np.transpose(X_outliers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Regularization\n",
    "X = np.r_[X_train,X_test]\n",
    "n_neighbors=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regulof(X):\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    clf = LocalOutlierFactor(n_neighbors=50)\n",
    "    y = clf.fit_predict(X)\n",
    "    xx, yy = np.meshgrid(np.linspace(-1, 1, 500), np.linspace(-1, 1, 500))\n",
    "    Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = X\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=50)\n",
    "neigh.fit(gs) \n",
    "NearestNeighbors(algorithm='auto', leaf_size=30)\n",
    "A  = neigh.kneighbors_graph(gs)\n",
    "A.toarray()\n",
    "\n",
    "\n",
    "distances, indices = neigh.kneighbors(gs)\n",
    "\n",
    "# 1 for inlier, -1 for outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the level sets of the decision function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rating individual files of train and dev on arousal and valence\n",
    "def listFilesds(basedir):\n",
    "    files = []\n",
    "    gs5 = [basedir +'/arousal/', basedir +'/valence/']    \n",
    "    #We get a tab countaining each file\n",
    "    for i, s in enumerate(gs5):\n",
    "        files.append([sorted(filter( lambda f: not f.startswith('.'), os.listdir(s+\".\"))),s])\n",
    "    return files;\n",
    "#End listFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write generated gold standard gs_5\n",
    "\n",
    "readdir ='gs_4' \n",
    "basedir = path \n",
    "header = \"\"\"@relation GOLDSTANDARD\n",
    "\n",
    "@attribute Instance_name string\n",
    "@attribute frameTime numeric\n",
    "@attribute GoldStandard numeric\n",
    "\n",
    "\n",
    "@data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "files = listFilesds(readdir)\n",
    "\n",
    "#X = np.zeros(6,7501)\n",
    "namespace = globals()\n",
    "for i in range(len(v.eName)/2):\n",
    "    for f in files[i][0]:\n",
    "        #print(files[i][1]+f)\n",
    "        #print((files[i][1]+f).replace(\"arousal\",\"valence\"))\n",
    "        tempr = pd.read_csv(files[i][1]+f,delimiter=';')\n",
    "        tempr = tempr['@relation GOLDSTANDARD'].apply(lambda x: x.split(',')[-1])\n",
    "        tempr = tempr[4:]\n",
    "        df_arr = pd.to_numeric(tempr)\n",
    "        \n",
    "        tempv = pd.read_csv((files[i][1]+f).replace(\"arousal\",\"valence\"),delimiter=';')\n",
    "        tempv = tempv['@relation GOLDSTANDARD'].apply(lambda x: x.split(',')[-1])\n",
    "        tempv = tempv[4:]\n",
    "        df_val = pd.to_numeric(tempv)\n",
    "        \n",
    "        fn = os.path.splitext(f)[0]\n",
    "        \n",
    "        X_temp = np.array([df_val,df_arr])\n",
    "        X_temp = np.transpose(X_temp)\n",
    "        X_re = np.r_[X,X_temp]\n",
    "        y = regulof(X_re)\n",
    "        \n",
    "#         X_arr = np.array((mfilt(df_arr.iloc[:,1]),mfilt(df_arr.iloc[:,2]),\\\n",
    "#                                      mfilt(df_arr.iloc[:,3]),mfilt(df_arr.iloc[:,4]),\\\n",
    "#                                      mfilt(df_arr.iloc[:,5]),mfilt(df_arr.iloc[:,6])))\n",
    "#         X_val = np.array((mfilt(df_val.iloc[:,1]),mfilt(df_val.iloc[:,2]),\\\n",
    "#                                      mfilt(df_val.iloc[:,3]),mfilt(df_val.iloc[:,4]),\\\n",
    "#                                      mfilt(df_val.iloc[:,5]),mfilt(df_val.iloc[:,6])))\n",
    "        \n",
    "#         [pca_aro, spca_aro] = spca_fn(X_arr)\n",
    "#         [pca_val, spca_val] = spca_fn(X_val)\n",
    "#         aggo_aro = agglo_fn(X_arr)\n",
    "#         aggo_val = agglo_fn(X_val)\n",
    "    \n",
    "#         arff_arr=open(basedir+\"arousal/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "#         arff_arr.write(header)\n",
    "#         for j in range(len(df_arr)):\n",
    "#             arff_arr.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(tempr[\"time\"].iloc[j])+\",\"+str(X_temp[j,0]))\n",
    "#         arff_arr.close()\n",
    "        \n",
    "#         arff_val=open(basedir+\"valence/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "#         arff_val.write(header)\n",
    "#         for j in range(len(df_val)):\n",
    "#             arff_val.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_val[\"time\"].iloc[j])+\",\"+str(X_temp[j,1]))\n",
    "#         arff_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat and hanning window\n",
    "def smooth(x,window_len=25*6,window='hanning'):\n",
    "\n",
    "    import numpy\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError, \"smooth only accepts 1 dimension arrays.\"\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError, \"Input vector needs to be bigger than window size.\"\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "\n",
    "\n",
    "    s=numpy.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=numpy.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('numpy.'+window+'(window_len)')\n",
    "        \n",
    "    y=numpy.convolve(w/w.sum(),x,mode='same')    \n",
    "    #y=numpy.convolve(w/w.sum(),s,mode='same')\n",
    "    return y#[(window_len/2-1):-(window_len/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write generated gold standard gs_5\n",
    "\n",
    "basedir = path \n",
    "header = \"\"\"@relation GOLDSTANDARD\n",
    "\n",
    "@attribute Instance_name string\n",
    "@attribute frameTime numeric\n",
    "@attribute GoldStandard numeric\n",
    "\n",
    "\n",
    "@data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "files = listFiles()\n",
    "\n",
    "#X = np.zeros(6,7501)\n",
    "namespace = globals()\n",
    "for i in range(len(v.eName)/2):\n",
    "    for f in files[i][0]:\n",
    "        #print(files[i][1]+f)\n",
    "        #print((files[i][1]+f).replace(\"arousal\",\"valence\"))\n",
    "        df_arr = pd.read_csv(files[i][1]+f,delimiter=';')\n",
    "        df_val = pd.read_csv((files[i][1]+f).replace(\"arousal\",\"valence\"),delimiter=';')\n",
    "        fn = os.path.splitext(f)[0]\n",
    "#         namespace['X_arr_%s' %fn] = df_arr.iloc[:,1:7]\n",
    "#         namespace['X_val_%s'%fn] = df_val.iloc[:,1:7]\n",
    "        \n",
    "#         X_arr = np.array((mfilt(df_arr.iloc[:,1]),mfilt(df_arr.iloc[:,2]),\\\n",
    "#                                      mfilt(df_arr.iloc[:,3]),mfilt(df_arr.iloc[:,4]),\\\n",
    "#                                      mfilt(df_arr.iloc[:,5]),mfilt(df_arr.iloc[:,6])))\n",
    "#         X_val = np.array((mfilt(df_val.iloc[:,1]),mfilt(df_val.iloc[:,2]),\\\n",
    "#                                      mfilt(df_val.iloc[:,3]),mfilt(df_val.iloc[:,4]),\\\n",
    "#                                      mfilt(df_val.iloc[:,5]),mfilt(df_val.iloc[:,6])))\n",
    "        \n",
    "#         [pca_aro, spca_aro] = spca_fn(X_arr)\n",
    "#         [pca_val, spca_val] = spca_fn(X_val)\n",
    "        aggo_aro = agglo_fn(X_arr)\n",
    "        aggo_val = agglo_fn(X_val)\n",
    "    \n",
    "        arff_arr=open(basedir+\"arousal/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_arr.write(header)\n",
    "        for j in range(len(aggo_aro)):\n",
    "            arff_arr.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_arr[\"time\"].iloc[j])+\",\"+str(aggo_aro[j,0]))\n",
    "        arff_arr.close()\n",
    "        \n",
    "        arff_val=open(basedir+\"valence/\"+f.split(\".\")[0]+\".arff\",\"w\")\n",
    "        arff_val.write(header)\n",
    "        for j in range(len(aggo_val)):\n",
    "            arff_val.write(\"\\n\"+f.split(\".\")[0]+\",\"+str(df_val[\"time\"].iloc[j])+\",\"+str(aggo_val[j,0]))\n",
    "        arff_val.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
